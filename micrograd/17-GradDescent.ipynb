{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "4ab52fa1-2bc4-48e8-a278-c72e9466ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "72ffc5ef-5513-417a-9008-ca21c9eb98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "1185ba83-0760-454f-8ada-714cdc6b81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, label='', _children=(), _op='', grad = 0, _backward = lambda: None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.grad = grad\n",
    "        self._backward = _backward\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        out = Value(self.data + other.data, _children = (self, other), _op = '+')\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(other, Value):\n",
    "            other = Value(other)\n",
    "        out = Value(self.data * other.data, _children = (self, other), _op= '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + -(other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return self + -(other)\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        if not isinstance(other, (int, float)):\n",
    "            raise TypeError(f\"Exponent must be an int or float, got {type(other).__name__}\")\n",
    "\n",
    "        out = Value(self.data ** other, _children=(self,), _op=f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * ( other * self.data ** (other - 1))\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "            \n",
    "    \n",
    "    def tanh(self):\n",
    "        x = (math.exp(2 * self.data) - 1)/ (math.exp(2 * self.data) + 1)\n",
    "        out = Value(x, _children = (self, ), _op = 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - (out.data ** 2)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out    \n",
    "        \n",
    "        \n",
    "\n",
    "    def backward(self):     \n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "          if v not in visited:\n",
    "            visited.add(v)\n",
    "            for child in v._prev:\n",
    "              build_topo(child)\n",
    "            topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "          node._backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "1d2fef7a-d333-4c06-ad43-6b61d538c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    # nin is the number of inputs to the neuron\n",
    "    def __init__(self, nin):\n",
    "        # generate weights & bias\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x will be a [x1, x2..]\n",
    "        # need to return activation_func((w1x1 + w2x2) + b)\n",
    "        dp = sum((xi * wi for xi, wi in zip(x, self.w)))\n",
    "        return (dp + self.b).tanh()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.b]+ self.w\n",
    "\n",
    "class Layer:\n",
    "    # nin: number of inputs\n",
    "    # nout: number of neurons\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out if len(out) > 1 else out[0] \n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for n in self.neurons:\n",
    "            params.extend(n.parameters())\n",
    "        return params\n",
    "\n",
    "class MLP:\n",
    "    # nin: number of inputs\n",
    "    # nouts: list of layers where each layer will define how many neurons it has\n",
    "    def __init__(self, nin, nouts):\n",
    "        # This is to create a list where each pair of elements will represent inputs <> ouputs\n",
    "        self.sz = [nin] + nouts\n",
    "        self.layers = [Layer(self.sz[i], self.sz[i+1]) for i in range(0, len(self.sz) - 1)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for l in self.layers:\n",
    "            params.extend(l.parameters())\n",
    "        return params\n",
    "\n",
    "    def clear_grad(self):\n",
    "        for param in self.parameters():\n",
    "            param.grad = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "f54b0967-70e6-4402-8641-4060a55341c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier, we were just focused on one set of inputs resulting in one output\n",
    "# Now, we will look at a bunch of inputs resulting in a bunch of outputs, and minimise the error rate by reducing loss\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "81cf9736-679a-4d62-8034-9ae04a206d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLP(3, [4, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "fb14e336-e650-498c-a5c0-a1fdf78af368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.01631754777502196\n",
      "Loss 0.01621564829754353\n",
      "Loss 0.01611491850713276\n",
      "Loss 0.016015339229101765\n",
      "Loss 0.01591689169355344\n",
      "Loss 0.01581955752497735\n",
      "Loss 0.0157233187321597\n",
      "Loss 0.015628157698396712\n",
      "Loss 0.015534057172001087\n",
      "Loss 0.015441000257091723\n",
      "Loss 0.015348970404655791\n",
      "Loss 0.01525795140387566\n",
      "Loss 0.015167927373710439\n",
      "Loss 0.015078882754723643\n",
      "Loss 0.014990802301149448\n",
      "Loss 0.01490367107318883\n",
      "Loss 0.014817474429528322\n",
      "Loss 0.014732198020073538\n",
      "Loss 0.014647827778891368\n",
      "Loss 0.01456434991735271\n",
      "Loss 0.014481750917470013\n",
      "Loss 0.014400017525423414\n",
      "Loss 0.01431913674526845\n",
      "Loss 0.014239095832820585\n",
      "Loss 0.014159882289709786\n",
      "Loss 0.014081483857600793\n",
      "Loss 0.014003888512572987\n",
      "Loss 0.013927084459655004\n",
      "Loss 0.013851060127509245\n",
      "Loss 0.013775804163261927\n",
      "Loss 0.01370130542747337\n",
      "Loss 0.013627552989244495\n",
      "Loss 0.013554536121455596\n",
      "Loss 0.013482244296133098\n",
      "Loss 0.01341066717993973\n",
      "Loss 0.013339794629785531\n",
      "Loss 0.013269616688554909\n",
      "Loss 0.013200123580946715\n",
      "Loss 0.013131305709423991\n",
      "Loss 0.013063153650269716\n",
      "Loss 0.012995658149745626\n",
      "Loss 0.012928810120350985\n",
      "Loss 0.012862600637178295\n",
      "Loss 0.012797020934362847\n",
      "Loss 0.0127320624016239\n",
      "Loss 0.01266771658089402\n",
      "Loss 0.012603975163034652\n",
      "Loss 0.01254082998463505\n",
      "Loss 0.012478273024892232\n",
      "Loss 0.012416296402569508\n",
      "Loss 0.012354892373031418\n",
      "Loss 0.012294053325352898\n",
      "Loss 0.012233771779500581\n",
      "Loss 0.012174040383583697\n",
      "Loss 0.012114851911173414\n",
      "Loss 0.01205619925868789\n",
      "Loss 0.011998075442841672\n",
      "Loss 0.011940473598157551\n",
      "Loss 0.011883386974538767\n",
      "Loss 0.011826808934900297\n",
      "Loss 0.011770732952857502\n",
      "Loss 0.011715152610470368\n",
      "Loss 0.011660061596041842\n",
      "Loss 0.01160545370196892\n",
      "Loss 0.011551322822644962\n",
      "Loss 0.011497662952411529\n",
      "Loss 0.011444468183559294\n",
      "Loss 0.011391732704375287\n",
      "Loss 0.011339450797236588\n",
      "Loss 0.0112876168367484\n",
      "Loss 0.011236225287925639\n",
      "Loss 0.01118527070441651\n",
      "Loss 0.01113474772676765\n",
      "Loss 0.01108465108072883\n",
      "Loss 0.011034975575597142\n",
      "Loss 0.010985716102598499\n",
      "Loss 0.010936867633306655\n",
      "Loss 0.010888425218097856\n",
      "Loss 0.01084038398464063\n",
      "Loss 0.010792739136419729\n",
      "Loss 0.010745485951293123\n",
      "Loss 0.010698619780081648\n",
      "Loss 0.01065213604518999\n",
      "Loss 0.01060603023925841\n",
      "Loss 0.010560297923844372\n",
      "Loss 0.010514934728133736\n",
      "Loss 0.010469936347679612\n",
      "Loss 0.010425298543169766\n",
      "Loss 0.010381017139220346\n",
      "Loss 0.010337088023196488\n",
      "Loss 0.010293507144058169\n",
      "Loss 0.01025027051123136\n",
      "Loss 0.010207374193503455\n",
      "Loss 0.010164814317942528\n",
      "Loss 0.010122587068839787\n",
      "Loss 0.0100806886866747\n",
      "Loss 0.010039115467102098\n",
      "Loss 0.009997863759961013\n",
      "Loss 0.00995692996830429\n"
     ]
    }
   ],
   "source": [
    "h = 0.01\n",
    "for i in range(1, 100):\n",
    "\n",
    "    # Generate predictions basis the current set of weights\n",
    "    ypred = [m(x) for x in xs]\n",
    "    \n",
    "    # loss is one number to quantify how far away we are from the prediction, using MSE here\n",
    "    loss = sum((ysi-ypredi)**2 for ysi, ypredi in zip(ys, ypred))\n",
    "    \n",
    "    # The objective is to get to a lower loss with each iteration\n",
    "    # To do this, we will move all the parameters in the direction of the gradient every time\n",
    "    # In this case, in the opposite direction since we want the loss to actually go down    \n",
    "    loss.backward() # Will generate all the gradients\n",
    "\n",
    "    for p in m.parameters():\n",
    "        p.data += -h * p.grad\n",
    "\n",
    "    m.clear_grad() # need to clear the grads as they accumulate\n",
    "\n",
    "    print(f'Loss {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "b4e6e0c5-e11a-4eb2-beed-7cf44ec52ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9551456599442291),\n",
       " Value(data=-0.963213084970007),\n",
       " Value(data=-0.933812461631954),\n",
       " Value(data=0.9529792514540648)]"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d4704-3402-44f5-8055-9cc525b9bdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (micrograd)",
   "language": "python",
   "name": "micrograd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
